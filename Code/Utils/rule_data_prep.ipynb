{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows:  [1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20]\n",
      "['ADS.fAHURHValveActADSInternalValuesMirror', 'ADS.fAHUTempODAADSInternalValuesMirror', 'ADS.fAHUFanSUPSpeedActADSInternalValuesMirror', 'ADS.fAHURHValveSetADSInternalValuesMirror', 'ADS.bAHUPHPumpOperatingADSInternalValuesMirror', 'ADS.fAHUPHValveActADSInternalValuesMirror', 'ADS.fAHUPHValveSetADSInternalValuesMirror', 'ADS.bAHURHPumpOperatingADSInternalValuesMirror', 'ADS.fAHUTempSUPADSInternalValuesMirror', 'ADS.fAHUFanSUPSpeedSetADSInternalValuesMirror', 'ADS.fAHUTempETAADSInternalValuesMirror', 'ADS.fAHUTempSUPSetADSInternalValuesMirror']\n",
      "(168, 480)\n",
      "(168, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create 'test' dataset for testing rules.\n",
    "# !!! This code saves only TEST .tsv files\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "data = pd.read_csv(r'/home/vaibhavs/Master_Thesis/ma-vaibhav/Data/data_prin_winter_15m_Nov23_Feb24.csv') # !!! CHANGE HERE (for new dataset)\n",
    "\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "dataset_name = 'AHU_prin_winter_2023_stanscaler_RULES' # !!! CHANGE HERE (for new dataset)\n",
    "dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "\n",
    "all_points = []\n",
    "all_labels = []\n",
    "all_windows = []\n",
    "datapoint_names = data.columns.tolist()\n",
    "datapoint_names.remove('time')\n",
    "minute_multiples = 480  # 480=5day, 1920=20day, 1440=1day(60s sampling)\n",
    "\n",
    "for datapoint in datapoint_names:\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    window_list = []\n",
    "    window_num = 0\n",
    "    dp_timeseries = data[['time',datapoint]] # isolate time-series of a single datapoint\n",
    "    rows = dp_timeseries.shape[0] # get number of timestamps\n",
    "    for i in range(0,rows,minute_multiples+1):\n",
    "        if i <= rows-minute_multiples:\n",
    "            sample = dp_timeseries.iloc[i:i+minute_multiples, [1]].transpose()\n",
    "            sample_list = sample.values.tolist()\n",
    "            # print(len(sample_list[0]))\n",
    "            data_list.append(sample_list[0])\n",
    "            window_num += 1\n",
    "            window_list.append(window_num)\n",
    "    \n",
    "    label_list = [datapoint] * len(data_list)\n",
    "    all_labels.append(label_list) \n",
    "    all_points.append(data_list)\n",
    "    all_windows.append(window_list)\n",
    "\n",
    "# Select a subset of windows to be in the test set\n",
    "total_windows = len(all_points[0])\n",
    "wins = np.arange(1, total_windows-1, 3)\n",
    "for val in wins:\n",
    "    wins = np.append(wins, val+1)\n",
    "wins = sorted(wins)\n",
    "print(\"Windows: \", wins)\n",
    "\n",
    "test_stubs = []\n",
    "window_stubs = []\n",
    "label_stubs = []\n",
    "for ind in range(len(all_points)):\n",
    "    for window in wins:   \n",
    "        test_stubs.append(all_points[ind][window])\n",
    "        window_stubs.append(all_windows[ind][window])\n",
    "        label_stubs.append(all_labels[ind][window])\n",
    "\n",
    "all_pointstubs = test_stubs\n",
    "all_labelstubs = label_stubs\n",
    "all_windowstubs = window_stubs\n",
    "print(list(set(all_labelstubs)))\n",
    "numpy_pointstubs = np.asarray(all_pointstubs)\n",
    "numpy_labelstubs = np.asarray(all_labelstubs)\n",
    "numpy_windowstubs = np.asarray(all_windowstubs)\n",
    "numpy_label_windows = np.column_stack((numpy_labelstubs, numpy_windowstubs)) # stack labels and their corresponding windows side-by-side\n",
    "\n",
    "print(numpy_pointstubs.shape)\n",
    "print(numpy_label_windows.shape)\n",
    "\n",
    "Xseries_septest = pd.DataFrame(data=numpy_pointstubs, columns=range(1,minute_multiples+1))\n",
    "Ylabels_septest = pd.DataFrame(data=numpy_label_windows)\n",
    "septest_merged = pd.concat([Ylabels_septest, Xseries_septest], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "septest_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_SEPTEST.tsv'), sep='\\t', index=False, header=None)\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "scaled_numpy_data = scaler.fit_transform(numpy_pointstubs)\n",
    "\n",
    "# # Min-Max Scaler\n",
    "# scaler = MinMaxScaler()\n",
    "# normalized_data = scaler.fit_transform(numpy_pointstubs)\n",
    "\n",
    "ltest = numpy_labelstubs.tolist()\n",
    "test_temp_labels = ['Temperature' if 'Temp' in x else x for x in ltest]\n",
    "test_valve_labels = ['Valve' if 'Valve' in x else x for x in test_temp_labels]\n",
    "test_speed_labels = ['Speed' if 'Speed' in x else x for x in test_valve_labels]\n",
    "test_oper_labels = ['Operating' if 'Operating' in x else x for x in test_speed_labels]\n",
    "\n",
    "Xseries_scaledtest = pd.DataFrame(data=scaled_numpy_data, columns=range(1,minute_multiples+1))\n",
    "Ylabels_scaledtest = pd.DataFrame(data=test_oper_labels)\n",
    "scaledtest_merged = pd.concat([Ylabels_scaledtest, Xseries_scaledtest], axis=1)\n",
    "scaledtest_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TEST_ed.tsv'), sep='\\t', index=False, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 'create_dataset' code for RULES dataset with dummy train and val; only test set exists\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"AHU_prin_winter_2023_stanscaler_RULES\" # !!! CHANGE HERE (for new dataset)\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "output_dir = './SimTSC/tmp'\n",
    "\n",
    "dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "df_test = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_TEST_ed.tsv'), sep='\\t', header=None)\n",
    "\n",
    "y_test = df_test.values[:, 0].astype(str)\n",
    "y = y_test\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "X_test = df_test.drop(columns=[0]).astype(np.float32)\n",
    "X_test.columns = range(X_test.shape[1])\n",
    "\n",
    "X_test = X_test.values\n",
    "X = X_test\n",
    "idx = np.array([i for i in range(len(X))])\n",
    "\n",
    "# np.random.shuffle(idx)\n",
    "# train_idx = idx[:int(len(idx)*0.15)] \n",
    "# val_idx = idx[int(len(idx)*0.15):int(len(idx)*0.7)]\n",
    "# test_idx = idx[int(len(idx)*0.7):]\n",
    "\n",
    "point_1 = len(X_test)\n",
    "\n",
    "test_idx = idx[:point_1]\n",
    "train_idx = 0\n",
    "val_idx = 0\n",
    "\n",
    "# add a dimension to make it multivariate with one dimension \n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "data = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'train_idx': train_idx,\n",
    "        'test_idx': test_idx,\n",
    "        'val_idx': val_idx\n",
    "}\n",
    "output_dir = os.path.join(output_dir, 'ebc_'+'1_shot')\n",
    "if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "np.save(os.path.join(output_dir, dataset_name), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69, 480)\n",
      "(69, 69)\n"
     ]
    }
   ],
   "source": [
    "# Distances for only the TEST data; (basically the 'create_dtw' script to save dtw/euclidean values)\n",
    "\n",
    "import sys\n",
    " \n",
    "# adding Folder_2 to the system path\n",
    "sys.path.insert(0, '/home/vaibhavs/Master_Thesis/ma-vaibhav/Code/SimTSC/pydtw')\n",
    "import dtw\n",
    "\n",
    "# Read dataset\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "dataset_name = 'StadtAachen_Winter_2023_stanscaler'\n",
    "output_dir = './SimTSC/tmp'\n",
    "\n",
    "euc_dir = os.path.join(output_dir, 'ebc_euclidean')\n",
    "if not os.path.exists(euc_dir):\n",
    "    os.makedirs(euc_dir)\n",
    "\n",
    "cid_dir = os.path.join(output_dir, 'ebc_cid')\n",
    "if not os.path.exists(cid_dir):\n",
    "    os.makedirs(cid_dir)\n",
    "\n",
    "dtw_dir = os.path.join(output_dir, 'ebc_dtw')\n",
    "if not os.path.exists(dtw_dir):\n",
    "    os.makedirs(dtw_dir)\n",
    "\n",
    "dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "# df_train = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_TRAIN.tsv'), sep='\\t', header=None)\n",
    "df_test = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_TEST_ed.tsv'), sep='\\t', header=None)\n",
    "# df_val = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_VAL.tsv'), sep='\\t', header=None)\n",
    "\n",
    "# X_train = df_train.drop(columns=[0]).astype(np.float32)\n",
    "X_test = df_test.drop(columns=[0]).astype(np.float32)\n",
    "# X_val = df_val.drop(columns=[0]).astype(np.float32)\n",
    "\n",
    "# X_train.columns = range(X_train.shape[1])\n",
    "X_test.columns = range(X_test.shape[1])\n",
    "# X_val.columns = range(X_val.shape[1])\n",
    "\n",
    "# X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "# X_val = X_val.values\n",
    "X = X_test\n",
    "\n",
    "X[np.isnan(X)] = 0\n",
    "std_ = X.std(axis=1, keepdims=True)\n",
    "std_[std_ == 0] = 1.0\n",
    "X = (X - X.mean(axis=1, keepdims=True)) / std_\n",
    "\n",
    "X = X.copy(order='C').astype(np.float64)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# # calculating DTW distances\n",
    "# distances = np.zeros((X.shape[0], X.shape[0]), dtype=np.float64)\n",
    "# for i in range(len(X)):\n",
    "#     for j in range(len(X)):\n",
    "#         data = X[i]\n",
    "#         query = X[j]\n",
    "#         distances[i][j] = dtw.query(data, query, r=min(len(data)-1, len(query)-1, 100))['value']\n",
    "\n",
    "# calculating Euclidean distances\n",
    "distances = np.zeros((X.shape[0], X.shape[0]), dtype=np.float64)\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X)):\n",
    "        data = X[i]\n",
    "        query = X[j]\n",
    "        distances[i][j] = np.linalg.norm(data - query)\n",
    "\n",
    "# # calculating CID distances\n",
    "# distances = np.zeros((X.shape[0], X.shape[0]), dtype=np.float64)\n",
    "# for i in range(len(X)):\n",
    "#     for j in range(len(X)):\n",
    "#         data = X[i]\n",
    "#         query = X[j]\n",
    "#         CE_Q = np.sqrt(np.sum(np.diff(data, axis=0)**2))\n",
    "#         CE_C = np.sqrt(np.sum(np.diff(query, axis=0)**2))\n",
    "#         distances[i][j] = np.sqrt(np.sum((data - query)**2)) * max(CE_Q, CE_C) / min(CE_Q, CE_C)\n",
    "        \n",
    "print(distances.shape)\n",
    "np.save(os.path.join(euc_dir, dataset_name), distances)\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simtsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
