{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample dictionary\n",
    "classification_dict = dict()\n",
    "classification_dict['FanSUPSpeedAct'] = {'substance':'speed', \n",
    "                                        'component':'Fan',\n",
    "                                        'subsystem':None}\n",
    "classification_dict['FanSUPSpeedSet'] = {'substance':'speed', \n",
    "                                        'component':'Fan',\n",
    "                                        'subsystem':None}\n",
    "classification_dict['fAHUTempEHAADS'] = {'substance':'temp', \n",
    "                                        'component':None,\n",
    "                                        'subsystem':None}\n",
    "classification_dict['fAHUTempODAADS'] = {'substance':'temp', \n",
    "                                        'component':None,\n",
    "                                        'subsystem':'ODA'}\n",
    "classification_dict['fAHUTempSUPADS'] = {'substance':'temp', \n",
    "                                        'component':None,\n",
    "                                        'subsystem':'SUP'}\n",
    "classification_dict['fAHUTempSUPSetADS'] = {'substance':'temp', \n",
    "                                        'component':None,\n",
    "                                        'subsystem':'SUP'}\n",
    "classification_dict['fAHUPHValveActADS'] = {'substance':'valve', \n",
    "                                            'component':'heater',\n",
    "                                            'subsystem':'ODA'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "dataset = \"AHU_prin_summer_2023_stanscaler_RULES\"\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "dataset_dir = os.path.join(dataset_dir, dataset)\n",
    "df_test = pd.read_csv(os.path.join(dataset_dir, dataset+'_SEPTEST.tsv'), sep='\\t', header=None)\n",
    "y_test = df_test.iloc[:,0:2]\n",
    "\n",
    "# Specify the filename of JSON file having all class predictions (substance, comp, subsys)\n",
    "read_filename = os.path.join(dataset+'_classes_dict.json')\n",
    "\n",
    "# Open the file in read mode with appropriate encoding (assuming UTF-8)\n",
    "with open(read_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "  # Use json.load to parse the JSON data from the file\n",
    "  classes_dict = json.load(f)\n",
    "\n",
    "# Get indices by column index\n",
    "category_indices = {}\n",
    "# for i in range(len(y_test.columns)):\n",
    "category = y_test.iloc[:, 0].unique() # Assuming the first unique value defines the category (might need adjustment)\n",
    "for point in category:\n",
    "    category_indices[point] = list(y_test.loc[y_test.iloc[:, 0] == point].index)\n",
    "\n",
    "# Define dict for storing point-wise classes i.e a triple of (substance, comp, subsys) for each datapoint\n",
    "pointwise_dict = dict()\n",
    "\n",
    "# Get the results\n",
    "for point, indices in category_indices.items():\n",
    "    # print(f\"Category: {point}, Indices: {indices}\")\n",
    "    \n",
    "    tmp_substance = []\n",
    "    tmp_comp = []\n",
    "    tmp_subsys = []\n",
    "    for index in indices:\n",
    "        tmp_substance.append(classes_dict['substance'][index])\n",
    "        tmp_comp.append(classes_dict['component'][index])\n",
    "        tmp_subsys.append(classes_dict['subsystem'][index])\n",
    "\n",
    "    # Create a Counter object to count element occurrences\n",
    "    substance_counts = Counter(tmp_substance)\n",
    "    comp_counts = Counter(tmp_comp)\n",
    "    subsys_counts = Counter(tmp_subsys)\n",
    "    # Find the most common value (assuming there's a single most frequent value)\n",
    "    most_common_substance = substance_counts.most_common(1)  # Get the most frequent element (limit to 1)\n",
    "    most_frequent_substance = most_common_substance[0][0]\n",
    "    most_common_comp = comp_counts.most_common(1)  # Get the most frequent element (limit to 1)\n",
    "    most_frequent_comp = most_common_comp[0][0]\n",
    "    most_common_subsys = subsys_counts.most_common(1)  # Get the most frequent element (limit to 1)\n",
    "    most_frequent_subsys = most_common_subsys[0][0]\n",
    "\n",
    "    pointwise_dict[point] = {'substance':most_frequent_substance, \n",
    "                            'component':most_frequent_comp,\n",
    "                            'subsystem':most_frequent_subsys}\n",
    "    \n",
    "for point, classes in pointwise_dict.items():\n",
    "    print(f\"Datapoint: {point}, Classes: {classes}\")\n",
    "\n",
    "# Open a file for writing in text mode (assuming UTF-8 encoding)\n",
    "save_filename = os.path.join(dataset+'_pointwise_preds_dict.json')\n",
    "with open(save_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Use json.dump to write the dictionary to the file\n",
    "    json.dump(pointwise_dict, f)\n",
    "\n",
    "print(f\"Dictionary saved to {save_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'soft' and 'hard' classification accuracy for rules\n",
    "\n",
    "import json\n",
    "\n",
    "dataset = \"AHU_prin_summer_2023_stanscaler_RULES\"\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "dataset_dir = os.path.join(dataset_dir, dataset)\n",
    "subs_test = pd.read_csv(os.path.join(dataset_dir, dataset+'_TEST_ed.tsv'), sep='\\t', header=None)\n",
    "subs_test = subs_test.iloc[:,0].tolist()\n",
    "comp_test = pd.read_csv(os.path.join(dataset_dir, dataset+'_TEST_comp.tsv'), sep='\\t', header=None)\n",
    "comp_test = comp_test.iloc[:,0].tolist()\n",
    "subsys_test = pd.read_csv(os.path.join(dataset_dir, dataset+'_TEST_subsys.tsv'), sep='\\t', header=None)\n",
    "subsys_test = subsys_test.iloc[:,0].tolist()\n",
    "\n",
    "# Specify the filename of JSON file having all class predictions (substance, comp, subsys)\n",
    "read_filename = os.path.join(dataset+'_classes_dict.json')\n",
    "\n",
    "# Open the file in read mode with appropriate encoding (assuming UTF-8)\n",
    "with open(read_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "  # Use json.load to parse the JSON data from the file\n",
    "  pred_classes_dict = json.load(f)\n",
    "\n",
    "subs_matches = [i for i, (a, b) in enumerate(zip(classes_dict['substance'], subs_test)) if str(a) == str(b)]\n",
    "print(len(subs_matches))\n",
    "comp_matches = [i for i, (a, b) in enumerate(zip(classes_dict['component'], comp_test)) if str(a) == str(b)]\n",
    "print(len(comp_matches))\n",
    "subsys_matches = [i for i, (a, b) in enumerate(zip(classes_dict['subsystem'], subsys_test)) if str(a) == str(b)]\n",
    "print(len(subsys_matches))\n",
    "\n",
    "# print non-matching pairs (true value, prediction) for Component class\n",
    "# not_comp_matches = [x for x in list(range(0,len(comp_test))) if x not in comp_matches]\n",
    "# for idx in not_comp_matches:\n",
    "#   print(f\"truth: {comp_test[idx]} & prediction: {classes_dict['component'][idx]}\")\n",
    "  \n",
    "total_len = len(comp_test)\n",
    "soft_counts = 0\n",
    "hard_counts = 0\n",
    "for i in range(0,total_len):\n",
    "  soft_counts += float(1/3)*(i in subs_matches) + float(1/3)*(i in comp_matches) + float(1/3)*(i in subsys_matches)\n",
    "  if i in subs_matches and i in comp_matches and i in subsys_matches:\n",
    "    hard_counts += 1\n",
    "\n",
    "soft_acc = (soft_counts / total_len)*100\n",
    "hard_acc = (hard_counts / total_len)*100\n",
    "print(f\"Soft accuracy: {soft_acc}%\")\n",
    "print(f\"Hard accuracy: {hard_acc}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1NN Classifier (Baseline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1NN classifier (baseline)\n",
    "\n",
    "# loading library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read dataset\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "dataset_name = 'AHU_principal_Winter_2023_unnorm'\n",
    "dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "df_train = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_TRAIN_RAW.tsv'), sep='\\t', header=None)\n",
    "df_test = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_TEST_RAW.tsv'), sep='\\t', header=None)\n",
    "df_val = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_VAL_RAW.tsv'), sep='\\t', header=None)\n",
    "\n",
    "Y_train = df_train[df_train.columns[0]].astype(np.str)\n",
    "Y_test = df_test[df_test.columns[0]].astype(np.str)\n",
    "Y_val = df_val[df_val.columns[0]].astype(np.str)\n",
    "\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "Y_val = Y_val.values\n",
    "Y = np.concatenate((Y_train, Y_test, Y_val), axis=0)\n",
    "\n",
    "X_train = df_train.drop(columns=[0]).astype(np.float32).values\n",
    "X_test = df_test.drop(columns=[0]).astype(np.float32).values\n",
    "X_val = df_val.drop(columns=[0]).astype(np.float32).values\n",
    "\n",
    "# instantiate learning model\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fitting the model\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# predict the response\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "\n",
    "print('\\nThe accuracy of the classifier is {}%'.format(accuracy_score(Y_test, pred)*100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
