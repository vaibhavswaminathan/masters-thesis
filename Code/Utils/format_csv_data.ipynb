{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is in multiple .CSV files, then they need to be combined together into a single dataframe. For this, run the code in the cell below. \n",
    "\n",
    "If the data is in a single .CSV file, skip the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14401, 9)\n",
      "(11233, 9)\n",
      "(25634, 9)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate multiple csv files (with same datapoint columns) to a single dataframe\n",
    "\n",
    "data_part1 = pd.read_csv(r'/home/vaibhavs/Master_Thesis/ma-vaibhav/Data/data_survey_15m_Nov22_Mar23.csv')\n",
    "data_part2 = pd.read_csv(r'/home/vaibhavs/Master_Thesis/ma-vaibhav/Data/data_survey_15m_Nov23_Feb24.csv')\n",
    "print(data_part1.shape)\n",
    "print(data_part2.shape)\n",
    "\n",
    "assert len(data_part1.columns) == len(data_part2.columns), f\"number of columns do not match\"\n",
    "\n",
    "data = pd.concat([data_part1, data_part2], axis=0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADS.bAHUPHPumpOperatingADSInternalValuesMirror', 'ADS.fAHUTempSUPSetADSInternalValuesMirror', 'ADS.fAHUPHValveSetADSInternalValuesMirror', 'ADS.fAHUTempODAADSInternalValuesMirror', 'ADS.fAHUPHValveActADSInternalValuesMirror', 'ADS.bAHURHPumpOperatingADSInternalValuesMirror', 'ADS.fAHUFanSUPSpeedActADSInternalValuesMirror', 'ADS.fAHURHValveSetADSInternalValuesMirror', 'ADS.fAHUFanSUPSpeedSetADSInternalValuesMirror', 'ADS.fAHUTempETAADSInternalValuesMirror', 'ADS.fAHUTempSUPADSInternalValuesMirror', 'ADS.fAHURHValveActADSInternalValuesMirror']\n",
      "(216, 480)\n"
     ]
    }
   ],
   "source": [
    "# Load AHU data from .csv file (generated by Aedifion) and convert into (n_samples X n_timestamps) format\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "data = pd.read_csv(r'/home/vaibhavs/Master_Thesis/ma-vaibhav/Data/data_prin_summer_15m_Jun23_Aug23.csv')\n",
    "\n",
    "all_points = []\n",
    "all_labels = []\n",
    "all_windows = []\n",
    "datapoint_names = data.columns.tolist()\n",
    "datapoint_names.remove('time')\n",
    "minute_multiples = 480  # 480=5day, 1920=20day, 96=1day, 1440=1day(60s sampling)\n",
    "\n",
    "for datapoint in datapoint_names:\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    window_list = []\n",
    "    window_num = 0\n",
    "    dp_timeseries = data[['time',datapoint]] # isolate time-series of a single datapoint\n",
    "    rows = dp_timeseries.shape[0] # get number of timestamps\n",
    "    for i in range(0,rows,minute_multiples+1):\n",
    "        if i <= rows-minute_multiples:\n",
    "            sample = dp_timeseries.iloc[i:i+minute_multiples, [1]].transpose()\n",
    "            sample_list = sample.values.tolist()\n",
    "            # print(len(sample_list[0]))\n",
    "            data_list.append(sample_list[0])\n",
    "            window_num += 1\n",
    "            window_list.append(window_num)\n",
    "    \n",
    "    label_list = [datapoint] * len(data_list)\n",
    "    all_labels.append(label_list) \n",
    "    all_points.append(data_list)\n",
    "    all_windows.append(window_list)\n",
    "\n",
    "all_points = list(itertools.chain.from_iterable(all_points))\n",
    "all_labels = list(itertools.chain.from_iterable(all_labels))\n",
    "\n",
    "orig_labels = all_labels.copy()\n",
    "\n",
    "# convert datapoint names to class labels\n",
    "# uniform_HR_labels = ['HR' if 'HR' in x else x for x in all_labels]\n",
    "# uniform_EHA_labels = ['EHA' if 'EHA' in x else x for x in uniform_HR_labels]\n",
    "# uniform_PH_labels = ['ODA' if 'PH' in x else x for x in uniform_EHA_labels]\n",
    "# uniform_ODA_labels = ['ODA' if 'ODA' in x else x for x in uniform_PH_labels]\n",
    "\n",
    "# all_labels = uniform_ODA_labels\n",
    "print(list(set(all_labels)))\n",
    "\n",
    "# check if number of labels (y) and number of time series (x) is equal\n",
    "assert len(all_points) == len(all_labels), f\"length of timeseries values list ({len(all_points)}) is not equal to length of datapoint labels list ({len(all_labels)}). Please make sure timeseries and their datapoint labels are of equal length\"\n",
    "\n",
    "# comb_labels = [all_labels, orig_labels]\n",
    "numpy_data = np.asarray(all_points)\n",
    "# numpy_labels = np.asarray(comb_labels)\n",
    "numpy_labels = np.asarray(all_labels)\n",
    "\n",
    "print(numpy_data.shape)\n",
    "\n",
    "# Standardize data with mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "scaled_numpy_data = scaler.fit_transform(numpy_data)\n",
    "\n",
    "# # Min-Max Scaler\n",
    "# scaler = MinMaxScaler()\n",
    "# normalized_data = scaler.fit_transform(numpy_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_numpy_data, numpy_labels, test_size=0.30, random_state=42, shuffle=True, stratify=numpy_labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.30, random_state=42, shuffle=True, stratify=y_train)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(scaled_numpy_data, numpy_labels, test_size=0.30, random_state=42, shuffle=False)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.30, random_state=42, shuffle=False)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(numpy_data, numpy_labels, test_size=0.30, random_state=42, shuffle=False)\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(X_train_raw, y_train_raw, test_size=0.30, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the folder name for saving the dataset files in the `dataset_name` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test datasets as .tsv files\n",
    "\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "dataset_name = 'AHU_principal_SUMMER_2023_stanscaler'\n",
    "\n",
    "dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "\n",
    "# Save train dataset\n",
    "Xseries_train = pd.DataFrame(data=X_train, columns=range(1,minute_multiples+1))\n",
    "Ylabels_train = pd.DataFrame(data=y_train)\n",
    "\n",
    "train_merged = pd.concat([Ylabels_train, Xseries_train], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "train_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TRAIN.tsv'), sep='\\t', index=False, header=None)\n",
    "\n",
    "# saving raw train\n",
    "Xseries_train_raw = pd.DataFrame(data=X_train_raw, columns=range(1,minute_multiples+1))\n",
    "Ylabels_train_raw = pd.DataFrame(data=y_train_raw)\n",
    "\n",
    "train_raw_merged = pd.concat([Ylabels_train_raw, Xseries_train_raw], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "train_raw_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TRAIN_RAW.tsv'), sep='\\t', index=False, header=None) \n",
    "\n",
    "# Save test dataset\n",
    "Xseries_test = pd.DataFrame(data=X_test, columns=range(1,minute_multiples+1))\n",
    "Ylabels_test = pd.DataFrame(data=y_test)\n",
    "\n",
    "test_merged = pd.concat([Ylabels_test, Xseries_test], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "test_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TEST.tsv'), sep='\\t', index=False, header=None)\n",
    "\n",
    "# saving raw test\n",
    "Xseries_test_raw = pd.DataFrame(data=X_test_raw, columns=range(1,minute_multiples+1))\n",
    "Ylabels_test_raw = pd.DataFrame(data=y_test_raw)\n",
    "\n",
    "test_raw_merged = pd.concat([Ylabels_test_raw, Xseries_test_raw], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "test_raw_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TEST_RAW.tsv'), sep='\\t', index=False, header=None) \n",
    "\n",
    "# Save validation dataset\n",
    "Xseries_val = pd.DataFrame(data=X_val, columns=range(1,minute_multiples+1))\n",
    "Ylabels_val = pd.DataFrame(data=y_val)\n",
    "\n",
    "val_merged = pd.concat([Ylabels_val, Xseries_val], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "val_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_VAL.tsv'), sep='\\t', index=False, header=None)\n",
    "\n",
    "# saving raw val\n",
    "Xseries_val_raw = pd.DataFrame(data=X_val_raw, columns=range(1,minute_multiples+1))\n",
    "Ylabels_val_raw = pd.DataFrame(data=y_val_raw)\n",
    "\n",
    "val_raw_merged = pd.concat([Ylabels_val_raw, Xseries_val_raw], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "val_raw_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_VAL_RAW.tsv'), sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below reads the dataset and assigns class labels to each datapoint based on the datapoint name. For e.g., the datapoint name 'fAHUTempEHAADS' will be assigned the class label 'Temperature'.\n",
    "\n",
    "The class labels currently defined are 'Temperature', 'Valve', 'Speed', and 'Operating'. Change them as per the class labels for your classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Operating', 'Temperature', 'Valve', 'Speed']\n",
      "(array(['Operating', 'Speed', 'Temperature', 'Valve'], dtype='<U11'), array([18, 17, 36, 34]))\n",
      "(array(['Operating', 'Speed', 'Temperature', 'Valve'], dtype='<U11'), array([10, 11, 21, 23]))\n",
      "(array(['Operating', 'Speed', 'Temperature', 'Valve'], dtype='<U11'), array([ 8,  8, 15, 15]))\n"
     ]
    }
   ],
   "source": [
    "# Transform datapoint names to class labels\n",
    "\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "dataset_name = 'AHU_principal_SUMMER_2023_stanscaler'\n",
    "\n",
    "dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "df_train = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_TRAIN.tsv'), sep='\\t', header=None)\n",
    "df_test = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_TEST.tsv'), sep='\\t', header=None)\n",
    "df_val = pd.read_csv(os.path.join(dataset_dir, dataset_name+'_VAL.tsv'), sep='\\t', header=None)\n",
    "\n",
    "ltrain = df_train.iloc[:,0].tolist()\n",
    "ltest = df_test.iloc[:,0].tolist()\n",
    "lval = df_val.iloc[:,0].tolist()\n",
    "\n",
    "# convert datapoint names to class labels\n",
    "train_temp_labels = ['Temperature' if 'Temp' in x else x for x in ltrain]\n",
    "train_valve_labels = ['Valve' if 'Valve' in x else x for x in train_temp_labels]\n",
    "train_speed_labels = ['Speed' if 'Speed' in x else x for x in train_valve_labels]\n",
    "train_oper_labels = ['Operating' if 'Operating' in x else x for x in train_speed_labels]\n",
    "\n",
    "print(list(set(train_oper_labels)))\n",
    "\n",
    "test_temp_labels = ['Temperature' if 'Temp' in x else x for x in ltest]\n",
    "test_valve_labels = ['Valve' if 'Valve' in x else x for x in test_temp_labels]\n",
    "test_speed_labels = ['Speed' if 'Speed' in x else x for x in test_valve_labels]\n",
    "test_oper_labels = ['Operating' if 'Operating' in x else x for x in test_speed_labels]\n",
    "\n",
    "val_temp_labels = ['Temperature' if 'Temp' in x else x for x in lval]\n",
    "val_valve_labels = ['Valve' if 'Valve' in x else x for x in val_temp_labels]\n",
    "val_speed_labels = ['Speed' if 'Speed' in x else x for x in val_valve_labels]\n",
    "val_oper_labels = ['Operating' if 'Operating' in x else x for x in val_speed_labels]\n",
    "\n",
    "df_train.iloc[:,0] = train_oper_labels\n",
    "df_test.iloc[:,0] = test_oper_labels\n",
    "df_val.iloc[:,0] = val_oper_labels\n",
    "\n",
    "print(np.unique(train_oper_labels, return_counts=True))\n",
    "print(np.unique(test_oper_labels, return_counts=True))\n",
    "print(np.unique(val_oper_labels, return_counts=True))\n",
    "\n",
    "df_train.to_csv(os.path.join(dataset_dir, dataset_name+'_TRAIN_ed.tsv'), sep='\\t', index=False, header=None)\n",
    "df_test.to_csv(os.path.join(dataset_dir, dataset_name+'_TEST_ed.tsv'), sep='\\t', index=False, header=None)\n",
    "df_val.to_csv(os.path.join(dataset_dir, dataset_name+'_VAL_ed.tsv'), sep='\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other miscellaneous code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using statistical functions to compute features (e.g., mean change, sample entropy, standard deviation) on the time series data, use the code cell below to: \n",
    "\n",
    "- Load .CSV data into a dataframe\n",
    "- Compute features on the time series data\n",
    "- Standardize the data and split it into train, test, validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADS.fAHUCOValveSetADSInternalValuesMirror', 'ADS.fAHUPHValveActADSInternalValuesMirror', 'ADS.fAHUPHValveSetADSInternalValuesMirror', 'ADS.fAHURHValveSetADSInternalValuesMirror', 'ADS.fAHUCOValveActADSInternalValuesMirror', 'ADS.fAHURHValveActADSInternalValuesMirror']\n",
      "(432, 6)\n"
     ]
    }
   ],
   "source": [
    "# Compute feature set from time-series samples\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tsfresh.feature_extraction.feature_calculators as tscalc\n",
    "\n",
    "data = pd.read_csv(r'/home/vaibhavs/Master_Thesis/ma-vaibhav/Data/data_principal_15m_Jan23_Feb24.csv')\n",
    "\n",
    "all_points = []\n",
    "all_labels = []\n",
    "all_windows = []\n",
    "datapoint_names = data.columns.tolist()\n",
    "datapoint_names.remove('time')\n",
    "minute_multiples = 480  # 480=5day, 1920=20day, 1440=1day(60s sampling)\n",
    "\n",
    "def get_features(sample):\n",
    "    features = []\n",
    "    abs_sum_changes = tscalc.absolute_sum_of_changes(sample)\n",
    "    cid_ce = tscalc.cid_ce(sample, True)\n",
    "    mean_change = tscalc.mean_change(sample)\n",
    "    reocc_vals = tscalc.percentage_of_reoccurring_values_to_all_values(sample)\n",
    "    sample_entropy = tscalc.sample_entropy(sample)\n",
    "    std = tscalc.standard_deviation(sample)\n",
    "    features = [abs_sum_changes, cid_ce, mean_change, reocc_vals, sample_entropy, std]\n",
    "    return features\n",
    "\n",
    "\n",
    "for datapoint in datapoint_names:\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    window_list = []\n",
    "    window_num = 0\n",
    "    dp_timeseries = data[['time',datapoint]] # isolate time-series of a single datapoint\n",
    "    rows = dp_timeseries.shape[0] # get number of timestamps\n",
    "    for i in range(0,rows,minute_multiples+1):\n",
    "        if i <= rows-minute_multiples:\n",
    "            sample = dp_timeseries.iloc[i:i+minute_multiples, [1]].transpose()\n",
    "            sample_list = sample.values.tolist()\n",
    "            features = get_features(sample_list[0])\n",
    "            data_list.append(features)\n",
    "            window_num += 1\n",
    "            window_list.append(window_num)\n",
    "    \n",
    "    label_list = [datapoint] * len(data_list)\n",
    "    all_labels.append(label_list) \n",
    "    all_points.append(data_list)\n",
    "    all_windows.append(window_list)\n",
    "\n",
    "all_points = list(itertools.chain.from_iterable(all_points))\n",
    "all_labels = list(itertools.chain.from_iterable(all_labels))\n",
    "\n",
    "print(list(set(all_labels)))\n",
    "\n",
    "# check if number of labels (y) and number of time series (x) is equal\n",
    "assert len(all_points) == len(all_labels), f\"length of timeseries values list ({len(all_points)}) is not equal to length of datapoint labels list ({len(all_labels)}). Please make sure timeseries and their datapoint labels are of equal length\"\n",
    "\n",
    "# comb_labels = [all_labels, orig_labels]\n",
    "numpy_data = np.asarray(all_points)\n",
    "# numpy_labels = np.asarray(comb_labels)\n",
    "numpy_labels = np.asarray(all_labels)\n",
    "\n",
    "print(numpy_data.shape)\n",
    "col_len = numpy_data.shape[1]\n",
    "\n",
    "# Standardize data with mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "scaled_numpy_data = scaler.fit_transform(numpy_data)\n",
    "\n",
    "# # Min-Max Scaler\n",
    "# scaler = MinMaxScaler()\n",
    "# normalized_data = scaler.fit_transform(numpy_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_numpy_data, numpy_labels, test_size=0.30, random_state=42, shuffle=True, stratify=numpy_labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.30, random_state=42, shuffle=True, stratify=y_train)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(numpy_data, numpy_labels, test_size=0.30, random_state=42, shuffle=True, stratify=numpy_labels)\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(X_train_raw, y_train_raw, test_size=0.30, random_state=42, shuffle=True, stratify=y_train_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Run ONLY for feature-computed dataset\n",
    "# Save train and test datasets as .tsv files\n",
    "\n",
    "dataset_dir = './SimTSC/datasets/EBC'\n",
    "dataset_name = 'AHU_valve_actset2023_features_stanscaler'\n",
    "\n",
    "dataset_dir = os.path.join(dataset_dir, dataset_name)\n",
    "\n",
    "# Save train dataset\n",
    "Xseries_train = pd.DataFrame(data=X_train, columns=range(1,col_len+1))\n",
    "Ylabels_train = pd.DataFrame(data=y_train)\n",
    "\n",
    "train_merged = pd.concat([Ylabels_train, Xseries_train], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "train_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TRAIN.tsv'), sep='\\t', index=False, header=None)\n",
    "\n",
    "# saving raw train\n",
    "Xseries_train_raw = pd.DataFrame(data=X_train_raw, columns=range(1,col_len+1))\n",
    "Ylabels_train_raw = pd.DataFrame(data=y_train_raw)\n",
    "\n",
    "train_raw_merged = pd.concat([Ylabels_train_raw, Xseries_train_raw], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "train_raw_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TRAIN_RAW.tsv'), sep='\\t', index=False, header=None) \n",
    "\n",
    "# Save test dataset\n",
    "Xseries_test = pd.DataFrame(data=X_test, columns=range(1,col_len+1))\n",
    "Ylabels_test = pd.DataFrame(data=y_test)\n",
    "\n",
    "test_merged = pd.concat([Ylabels_test, Xseries_test], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "test_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TEST.tsv'), sep='\\t', index=False, header=None)\n",
    "\n",
    "# saving raw test\n",
    "Xseries_test_raw = pd.DataFrame(data=X_test_raw, columns=range(1,col_len+1))\n",
    "Ylabels_test_raw = pd.DataFrame(data=y_test_raw)\n",
    "\n",
    "test_raw_merged = pd.concat([Ylabels_test_raw, Xseries_test_raw], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "test_raw_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_TEST_RAW.tsv'), sep='\\t', index=False, header=None) \n",
    "\n",
    "# Save validation dataset\n",
    "Xseries_val = pd.DataFrame(data=X_val, columns=range(1,col_len+1))\n",
    "Ylabels_val = pd.DataFrame(data=y_val)\n",
    "\n",
    "val_merged = pd.concat([Ylabels_val, Xseries_val], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "val_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_VAL.tsv'), sep='\\t', index=False, header=None)\n",
    "\n",
    "# saving raw val\n",
    "Xseries_val_raw = pd.DataFrame(data=X_val_raw, columns=range(1,col_len+1))\n",
    "Ylabels_val_raw = pd.DataFrame(data=y_val_raw)\n",
    "\n",
    "val_raw_merged = pd.concat([Ylabels_val_raw, Xseries_val_raw], axis=1)\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "val_raw_merged.to_csv(os.path.join(dataset_dir, dataset_name+'_VAL_RAW.tsv'), sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prediction labels as list from pickle file\n",
    "import pickle\n",
    "seed = 0\n",
    "dataset = \"AHU_Minimal_2023\"\n",
    "log_dir = './SimTSC/logs'\n",
    "test_out_dir = os.path.join(log_dir, 'TEST')\n",
    "preds_out_path = os.path.join(test_out_dir, dataset+'_'+str(seed)+'_preds')\n",
    "with open (preds_out_path, 'rb') as fp:\n",
    "    labellist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODATemp rule holds overall --> Accuracy: 0.00\n",
      "Precedent holds for 99.54 percent of all samples\n",
      "Consequent holds 0.00 percent when precedent holds\n"
     ]
    }
   ],
   "source": [
    "log_dir = './SimTSC/logs'\n",
    "tmp_dir = 'tmp'\n",
    "dataset = \"data_principal_plus1_15m_Jan23_Dec23\"\n",
    "dataset_dir = '../Data'\n",
    "\n",
    "# dataset_dir = os.path.join(dataset_dir, dataset)\n",
    "data = pd.read_csv(os.path.join(dataset_dir, dataset+'.csv'))\n",
    "data.head()\n",
    "\n",
    "ODAtemp_idx = [idx for idx, x in enumerate(data.columns) if \"TempODA\" in x]\n",
    "SUPtemp_idx = [idx for idx, x in enumerate(data.columns) if \"TempSUPADS\" in x]\n",
    "SUPset_idx = [idx for idx, x in enumerate(data.columns) if \"TempSUPSet\" in x]\n",
    "RHvalve_idx = [idx for idx, x in enumerate(data.columns) if \"RHValve\" in x]\n",
    "RHpump_idx = [idx for idx, x in enumerate(data.columns) if \"RHPump\" in x]\n",
    "COtemp_idx = [idx for idx, x in enumerate(data.columns) if \"COTempOut\" in x]\n",
    "PHvalve_idx = [idx for idx, x in enumerate(data.columns) if \"PHValveAct\" in x]\n",
    "PHpump_idx = [idx for idx, x in enumerate(data.columns) if \"PHPump\" in x]\n",
    "\n",
    "# If SUP_set - ODA_temp > 6°C --> RH_valve > 20%\n",
    "# np.abs(data.iloc[idx,SUPset_idx][0] - data.iloc[idx,ODAtemp_idx][0]) > 6.0 and data.iloc[idx,RHvalve_idx][0] > 20.0\n",
    "\n",
    "# If SUP_temp - CO_temp_out > 1°K --> RH_Pump = 1.0\n",
    "# data.iloc[idx,SUPtemp_idx][0] - data.iloc[idx,COtemp_idx][0] > 1.0 and data.iloc[idx,RHpump_idx][0] == 1.0\n",
    "\n",
    "# ODATemp < 3°C implies PH ValveAct > 0%\n",
    "\n",
    "\n",
    "nsamples = data.shape[0]\n",
    "count = 0\n",
    "precedent = 0\n",
    "consequent = 0\n",
    "for idx in range(nsamples):\n",
    "    if -10.0 <= data.iloc[idx,ODAtemp_idx][0] <= 30.0:\n",
    "        precedent+=1\n",
    "        # if data.iloc[idx,PHpump_idx][0] == 0.0:\n",
    "        #     consequent+=1\n",
    "\n",
    "precision = (consequent / nsamples)*100\n",
    "cons_acc = (consequent / precedent)*100\n",
    "prec_acc = (precedent / nsamples)*100\n",
    "print(\"ODATemp rule holds overall --> Accuracy: {:.2f}\".format(precision))\n",
    "print(\"Precedent holds for {:.2f} percent of all samples\".format(prec_acc))\n",
    "print(\"Consequent holds {:.2f} percent when precedent holds\".format(cons_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simtsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
